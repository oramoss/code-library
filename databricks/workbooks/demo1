# Youtube video: https://www.youtube.com/watch?v=M7t1T1Q5MNc
# Demo of databricks on azure
# Requires a test parquet file in the relevant azure ADLS2 storage location

# python!

#Set up some variables...change as necessary
blob_account_name = "<< the storage account name>>"
blob_container_name = "<<the container name>>"
blob_relative_path = "parquet"
blob_sas_token = "<<insert SAS token here>>"


#Set up the path to the files location
wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name,blob_account_name,blob_relative_path)
spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name,blob_account_name),blob_sas_token)
print ('Remote blob path: ' + wasbs_path)


#read the parquet file
df = spark.read.parquet(wasbs_path)

#Register the data frame as a temporary view
print('Register the data frame as a SQL temporary view: source')
df.createOrReplaceTempView('source')

#count the records
df.count()

#Get the first 10 rows using SQL
%sql
SELECT * FROM source LIMIT 10

